{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline Local - Exécution Interactive\n",
    "\n",
    "Ce notebook exécute le pipeline complet de détection de fraude en mode local, cellule par cellule, pour permettre l'inspection intermédiaire des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1/6] Initialisation de la Session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 18:32:32 WARN Utils: Your hostname, laureal-Inspiron-7306-2n1 resolves to a loopback address: 127.0.1.1; using 192.168.1.22 instead (on interface wlp0s20f3)\n",
      "25/11/06 18:32:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/06 18:32:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/06 18:32:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Session Spark initialisée (Version: 3.4.1, Master: local[*])\n"
     ]
    }
   ],
   "source": [
    "pipeline_start_time = time.time()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionPipeline-local\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"--add-opens=java.base/java.lang=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"--add-opens=java.base/java.lang=ALL-UNNAMED\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f\"✓ Session Spark initialisée (Version: {sc.version}, Master: {sc.master})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2/6] Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé: 284807 lignes, 31 colonnes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 18:32:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 228045 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 56762 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/raw/creditcard.csv\"\n",
    "\n",
    "initial_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Dataset chargé: {initial_df.count()} lignes, {len(initial_df.columns)} colonnes\")\n",
    "\n",
    "# Division train/test (80/20)\n",
    "train_df, test_df = initial_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Train: {train_df.count()} lignes\")\n",
    "print(f\"Test: {test_df.count()} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des Données d'Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n",
      "\n",
      "Distribution de la cible en entraînement :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   400|\n",
      "|    0|227645|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()\n",
    "\n",
    "class_dist = train_df.groupBy(\"Class\").agg(count(\"*\").alias(\"count\"))\n",
    "print(\"\\nDistribution de la cible en entraînement :\")\n",
    "class_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3/6] Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes de features : ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "Pipeline créé. Lancement de l'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle entraîné en 13.10 secondes.\n"
     ]
    }
   ],
   "source": [
    "# Récupérer tous les noms de colonnes sauf 'Class' (la cible)\n",
    "feature_cols = [col for col in train_df.columns if col != 'Class']\n",
    "\n",
    "print(f\"Colonnes de features : {feature_cols}\")\n",
    "\n",
    "# Créer un vecteur de features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Normaliser les features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Créer le modèle RandomForest\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"Class\",\n",
    "    numTrees=10,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Créer le pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "print(\"Pipeline créé. Lancement de l'entraînement...\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "training_end_time = time.time()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"✓ Modèle entraîné en {training_duration:.2f} secondes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4/6] Évaluation des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions effectuées.\n",
      "+-----+----------+--------------------+\n",
      "|Class|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    0|       0.0|[0.99994613603415...|\n",
      "|    0|       0.0|[0.99993223754481...|\n",
      "|    0|       0.0|[0.99986023838505...|\n",
      "|    0|       0.0|[0.99964341814314...|\n",
      "|    0|       0.0|[0.99994840470464...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions sur le jeu de test\n",
    "predictions_df = pipeline_model.transform(test_df)\n",
    "\n",
    "print(\"Prédictions effectuées.\")\n",
    "predictions_df.select(\"Class\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MÉTRIQUES DE PERFORMANCE ===\n",
      "Accuracy: 0.9995\n",
      "Precision: 0.9995\n",
      "Recall: 0.9995\n",
      "F1-Score: 0.9995\n",
      "AUC-ROC: 0.9769\n",
      "AUC-PR: 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Évaluateur binaire (pour AUC-PR et AUC-ROC)\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\")\n",
    "auc_roc = binary_evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions_df)\n",
    "auc_pr = binary_evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions_df)\n",
    "\n",
    "# Évaluateur multi-classe (pour accuracy, precision, recall, f1)\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions_df)\n",
    "precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions_df)\n",
    "recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(predictions_df)\n",
    "f1 = multi_evaluator.setMetricName(\"f1\").evaluate(predictions_df)\n",
    "\n",
    "print(f\"\\n=== MÉTRIQUES DE PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {auc_pr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5/6] Sauvegarde des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métriques sauvegardées dans : ../results/metrics_local_notebook.json\n",
      "✓ Modèle sauvegardé dans : ../models/fraud_detection_model_local_notebook.spark\n"
     ]
    }
   ],
   "source": [
    "# Créer le dictionnaire de métriques\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'areaUnderROC': auc_roc,\n",
    "    'areaUnderPR': auc_pr,\n",
    "    'total_execution_time_seconds': round(time.time() - pipeline_start_time, 2),\n",
    "    'training_time_seconds': round(training_duration, 2)\n",
    "}\n",
    "\n",
    "# Créer les dossiers si nécessaire\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Sauvegarder les métriques en JSON\n",
    "metrics_path = \"../results/metrics_local_notebook.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"✓ Métriques sauvegardées dans : {metrics_path}\")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model_path = \"../models/fraud_detection_model_local_notebook.spark\"\n",
    "pipeline_model.write().overwrite().save(model_path)\n",
    "print(f\"✓ Modèle sauvegardé dans : {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6/6] Arrêt de la Session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PIPELINE TERMINÉ AVEC SUCCÈS ===\n",
      "Temps total d'exécution : 79.25 secondes\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"\\n=== PIPELINE TERMINÉ AVEC SUCCÈS ===\")\n",
    "print(f\"Temps total d'exécution : {metrics['total_execution_time_seconds']:.2f} secondes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-fraud-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
